{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer consists of an encoder and decoder part:\n",
    "\n",
    "<img src=\"transformer_architecture.png\" width=\"500\">\n",
    "\n",
    "Input is embeded into a vector representation and fed into the encoder. Embeding can be learnt or we can use one of the basic embeding options.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder\n",
    "\n",
    "Encoder is built from N stacked encoder blocks.\n",
    "\n",
    "<img src=\"encoder_block.png\">\n",
    "\n",
    "Each encoder block has the output size of d_model which in paper is set to 512.\n",
    "Encoder block consists of:\n",
    "- multi head attention block\n",
    "- fully connected feed forward block\n",
    "- layer normalization following each of the sub-layers\n",
    "\n",
    "Input to each sub-block is added to the output of sub-block with a residual connection (evident with the arrow connection around the element) so output of each layer is $LayerNorm(x+Sublayer(x))$. All the sub-layers and embedding layer produce the same dimension of d_model tensors, to allow for the residual connections."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention\n",
    "\n",
    "Attention according to the paper is a mapping from a query and a set of key-value pairs to an output all of which are vectors. Mapping is calculated as a weighted sum of the values, where the weights are obtained by computing a compatibility function of the query and corresponding key.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"attention.png\">\n",
    "    <figcaption>A represents the compatibility function between query and key; B is applying the weight to the corresponding value</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "Input is queries and keys of dimension $d_{k}$ and values of dimension $d_{v}$. Formula for a single query $q$ is\n",
    "$$\n",
    "Attention(q,K,V) = softmax(\\frac{qK^{T}}{\\sqrt{d_{k}}})V\n",
    "$$\n",
    "where q is a single query, K is all keys in matrix form and V is all values in matrix form. This is a simple dot-product attention that gets scaled with $\\sqrt{d_{k}}$ to keep the speed of optimized dot product and save the dot-product from getting too large for softmax. This manoeuvre brings it in line with additive attention while making it much more computationally efficient.\n",
    "\n",
    "Values, keys and queries are all calculated with their respective weighted matrices eg: $q = W_{q}x$ where q is query vector, x is embeding vector of input sequence and $W_{q}$ is the query learnable weight matrix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    #d_k = d_K/n_heads\n",
    "    def __init__(self,d_model,d_k):\n",
    "        super().__init__()\n",
    "        self.weightQ = torch.nn.Linear(d_model,d_k,bias = False)\n",
    "        self.weightK = torch.nn.Linear(d_model,d_k,bias = False)\n",
    "        self.weightV = torch.nn.Linear(d_model,d_k,bias = False)\n",
    "\n",
    "    def forward(self,incoming):\n",
    "        query = self.weightQ(incoming)\n",
    "        key = self.weightK(incoming)\n",
    "        value = self.weightV(incoming)\n",
    "        inter = torch.nn.functional.softmax(torch.bmm(query,torch.transpose(key,1,2))/math.sqrt(query.size(dim=2)),dim=-1)\n",
    "        return torch.bmm(inter,value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 64])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Attention(512,int(512/8))\n",
    "input = torch.randn((16,1,512))\n",
    "ret = test.forward(input.float())\n",
    "ret.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiHeadAttention\n",
    "\n",
    "<img src=\"multiHead.png\">\n",
    "\n",
    "We stack $n\\_heads$ attention heads together and concatenate their outputs. The concatenated output is then projected for the final time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,d_model = 512,n_heads = 8):\n",
    "        super().__init__()\n",
    "        self.d_k = int(d_model/n_heads)\n",
    "        self.d_model = d_model\n",
    "        assert self.d_k*n_heads == d_model , \"Embed dimension is not divisible with amoutn of heads\"\n",
    "\n",
    "        self.heads = torch.nn.ModuleList(\n",
    "            [Attention(self.d_model,self.d_k)]*n_heads\n",
    "        )\n",
    "        self.out_lin_layer = torch.nn.Linear(d_model,d_model)\n",
    "\n",
    "    def forward(self,incoming):\n",
    "        concat_out = torch.cat([x(incoming) for x in self.heads],dim = -1)\n",
    "        return self.out_lin_layer(concat_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 512])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_h = MultiHeadAttention()\n",
    "ret = multi_h.forward(torch.randn(16,1,512))\n",
    "ret.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class FullyConnectedFeedForwardNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,d_model=512,d_hidden = 2048, dropout_prob = 0.5):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model,d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden,d_model),\n",
    "            torch.nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self,incoming):\n",
    "        return self.model(incoming)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 512])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FullyConnectedFeedForwardNetwork()\n",
    "ret = ffn.forward(torch.randn(16,1,512))\n",
    "ret.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}